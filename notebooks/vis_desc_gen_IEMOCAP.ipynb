{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 12:06:43.388642: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-14 12:06:43.395711: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-14 12:06:43.404699: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-14 12:06:43.407463: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-14 12:06:43.414249: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-14 12:06:43.860270: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import copy\n",
    "import requests\n",
    "import warnings\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from moviepy.editor import VideoFileClip\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "%matplotlib inline\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/shareds/IEMOCAP/IEMOCAP_full_release/'\n",
    "save_path = '/shareds/IEMOCAP/IEMOCAP_video'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion2short = {'Neutral': 'neu', \n",
    "                 'Excited': 'exc', \n",
    "                 'Frustration': 'fru', \n",
    "                 'Sadness': 'sad', \n",
    "                 'Anger': 'ang', \n",
    "                 'Happiness': 'hap' \n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_video(video_path, start_time, end_time, output_path):\n",
    "    video = VideoFileClip(video_path)\n",
    "    start_sec = float(start_time) + 2 / 100\n",
    "    end_sec = float(end_time) + 2 / 100\n",
    "    video_subclip = video.subclip(start_sec, min(video.duration, end_sec))\n",
    "    video_subclip.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\", verbose=False, logger=None)\n",
    "\n",
    "\n",
    "def get_emotion(label):\n",
    "    emotions = dict()\n",
    "    for row in ''.join(label).split('\\n\\n')[1:-1]:\n",
    "        row = row.split('\\n')\n",
    "        head = row[0].split('\\t')\n",
    "        name = head[1]\n",
    "        emo = head[2]\n",
    "        if emo not in list(emotion2short.values()):\n",
    "            answers = ''\n",
    "            for e in row[1:]:\n",
    "                if e[0] == 'C':\n",
    "                    answers += e.split('\\t')[1] + ' '\n",
    "            answers = answers.split('; ')\n",
    "            answers = [e for e in answers if e in emotion2short]\n",
    "            if answers:\n",
    "                emo = emotion2short[max(set(answers), key=answers.count)]\n",
    "            else:\n",
    "                emo = np.nan\n",
    "        emotions[name] = emo\n",
    "    return emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 5/5 [1:08:14<00:00, 818.98s/it]\n"
     ]
    }
   ],
   "source": [
    "label_pattern = r'^(\\S+)\\s+\\[([0-9]+\\.[0-9]+)-([0-9]+\\.[0-9]+)\\]:\\s*(.*)$'\n",
    "\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "markup = pd.DataFrame(columns=['session', 'fn', 'idx', 'sex', 'emotion', 'text'])\n",
    "for i in tqdm(range(1, 6)):\n",
    "    os.makedirs(f'{save_path}/Session{i}', exist_ok=True)\n",
    "    for fn in os.listdir(f'{data_path}Session{i}/dialog/transcriptions/'):\n",
    "        if fn[0] == '.':\n",
    "            continue\n",
    "        os.makedirs(f'{save_path}/Session{i}/{fn[:-4]}', exist_ok=True)\n",
    "        \n",
    "        with open(f'{data_path}/Session{i}/dialog/transcriptions/{fn}', 'r') as f:\n",
    "            labels = f.readlines()\n",
    "\n",
    "        with open(f'{data_path}/Session{i}/dialog/EmoEvaluation/{fn}', 'r') as f:\n",
    "            emotions = f.readlines()\n",
    "            emotions = get_emotion(emotions)\n",
    "\n",
    "        for j, label in enumerate(labels):\n",
    "            match = re.match(label_pattern, label)\n",
    "            if match is None:\n",
    "                # print(f\"Invalid label: {label}\")\n",
    "                continue\n",
    "            row_name, start_time, end_time, text = [match.group(i) for i in range(1, 5)]\n",
    "\n",
    "            video_path = f'{data_path}Session{i}/dialog/avi/DivX/{fn[:-4]}.avi'\n",
    "            if not os.path.isfile(video_path):\n",
    "                print(f\"No such file or directory: {video_path}\")\n",
    "                continue\n",
    "        \n",
    "            video_save_path = f'{save_path}/Session{i}/{fn[:-4]}/{j}.avi'   \n",
    "\n",
    "            if label.split(' ')[0] in emotions:\n",
    "                clip_video(video_path, start_time, end_time, video_save_path)\n",
    "                markup.loc[markup.shape[0]] = [i, fn[:-4], j, row_name[-4], emotions[label.split(' ')[0]], text]\n",
    "markup = markup.dropna()\n",
    "markup.to_csv(f'{save_path}/markup.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad520b810064078b53c8ef646fad60d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\", device_map=\"auto\", load_in_4bit=True)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>fn</th>\n",
       "      <th>idx</th>\n",
       "      <th>sex</th>\n",
       "      <th>emotion</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Ses01F_impro07</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "      <td>exc</td>\n",
       "      <td>Did you get the letter?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ses01F_impro07</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>exc</td>\n",
       "      <td>Yes.  There's a big envelope it says, you're i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Ses01F_impro07</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>exc</td>\n",
       "      <td>Yeah. That is so awesome.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Ses01F_impro07</td>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>exc</td>\n",
       "      <td>Oh my God. What are you going to do? [LAUGHTER]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Ses01F_impro07</td>\n",
       "      <td>5</td>\n",
       "      <td>F</td>\n",
       "      <td>exc</td>\n",
       "      <td>So I have to move back to the ghetto but...I k...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session              fn  idx sex emotion  \\\n",
       "0        1  Ses01F_impro07    0   M     exc   \n",
       "1        1  Ses01F_impro07    1   F     exc   \n",
       "2        1  Ses01F_impro07    2   M     exc   \n",
       "3        1  Ses01F_impro07    4   M     exc   \n",
       "4        1  Ses01F_impro07    5   F     exc   \n",
       "\n",
       "                                                text  \n",
       "0                            Did you get the letter?  \n",
       "1  Yes.  There's a big envelope it says, you're i...  \n",
       "2                          Yeah. That is so awesome.  \n",
       "3    Oh my God. What are you going to do? [LAUGHTER]  \n",
       "4  So I have to move back to the ghetto but...I k...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(f'{save_path}/markup.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_first_frame(video_path, resize_shape=(224, 224)):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    if not ret:\n",
    "        return None\n",
    "    \n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image = Image.fromarray(frame)\n",
    "\n",
    "    if resize_shape:\n",
    "        image = image.resize(resize_shape, Image.Resampling.LANCZOS)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 437/1253 [4:15:35<8:03:43, 35.57s/it] "
     ]
    }
   ],
   "source": [
    "batch_size = 8                    \n",
    "max_new_tokens = 256  \n",
    "video_desc = [\"Nan\"] * len(data) \n",
    "\n",
    "num_rows = len(data)\n",
    "for start_idx in tqdm(range(0, num_rows, batch_size), total=(num_rows // batch_size + 1)):\n",
    "    end_idx = min(start_idx + batch_size, num_rows)\n",
    "    \n",
    "    batch = data.iloc[start_idx:end_idx]\n",
    "\n",
    "    batch_images = []\n",
    "    batch_texts = []\n",
    "    \n",
    "    for i, row in batch.iterrows():\n",
    "        sess, file, local_idx, emotion, *rest = row\n",
    "        video_path = f\"{save_path}/Session{sess}/{file}/{local_idx}.avi\"\n",
    "        \n",
    "        image = extract_first_frame(video_path)\n",
    "        if image is None:\n",
    "            batch_images.append(None)\n",
    "            batch_texts.append(None)\n",
    "            continue\n",
    "        \n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a helpful assistant that provides a thorough analysis of the image. \"\n",
    "                    \"Focus on describing what is visible, including objects, background, color palette, \"\n",
    "                    \"composition, mood, and any context clues. \"\n",
    "                    \"Provide a detailed multi-sentence description rather than a single-sentence answer. \"\n",
    "                    \"Focus on what can be directly observed.\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": image},\n",
    "                    {\"type\": \"text\", \"text\": (\n",
    "                        \"Please describe in detail what you see in this image. \"\n",
    "                        \"Comment on the objects, the overall context, and any visual characteristics. \"\n",
    "                        \"Use at least three sentences.\"\n",
    "                    )}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        text_for_model = processor.apply_chat_template(\n",
    "            conversation,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False\n",
    "        )\n",
    "        \n",
    "        batch_images.append(image)\n",
    "        batch_texts.append(text_for_model)\n",
    "    \n",
    "    valid_indices = [idx for idx, img in enumerate(batch_images) if img is not None]\n",
    "    if len(valid_indices) != 0:\n",
    "\n",
    "        all_images = [batch_images[i] for i in valid_indices]\n",
    "        all_texts  = [batch_texts[i]  for i in valid_indices]\n",
    "    \n",
    "        inputs = processor(\n",
    "            text=all_texts,\n",
    "            images=all_images,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).to(model.device)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            generate_ids = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "        \n",
    "        prompt_length = inputs.input_ids.shape[1]\n",
    "        generate_ids = generate_ids[:, prompt_length:]\n",
    "        \n",
    "        responses = processor.batch_decode(\n",
    "            generate_ids,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=False\n",
    "        )\n",
    "    \n",
    "        for idx_local, resp in zip(valid_indices, responses):\n",
    "            video_desc[idx_local] = resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"video_caption\"] = video_desc\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"../data/IEMOCAP/modified_video_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1182338,
     "sourceId": 1978289,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
